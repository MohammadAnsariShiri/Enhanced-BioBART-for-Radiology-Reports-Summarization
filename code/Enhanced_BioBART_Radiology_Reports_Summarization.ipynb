{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eaIuPOSyCqdJ"
      },
      "outputs": [],
      "source": [
        "# (Cell 1) Install Libraries\n",
        "\n",
        "!pip install nltk rouge-score py-rouge transformers tqdm datasets evaluate scispacy negspacy\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz\n",
        "! python -m nltk.downloader punkt stopwords\n",
        "! pip install rouge_score evaluate\n",
        "! pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LonDEIL9c5y"
      },
      "outputs": [],
      "source": [
        "# (Cell 2) Import Modules\n",
        "# Import core libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration # Using BART model\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.auto import tqdm # Use auto version for better notebook integration\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # For TF-IDF target generation\n",
        "import gc  # Garbage collector\n",
        "import os\n",
        "import json\n",
        "import re # For section parsing\n",
        "import time # For runtime tracking\n",
        "import random # For selecting qualitative examples\n",
        "from torch.cuda.amp import autocast, GradScaler # For Mixed Precision Training\n",
        "from collections import Counter\n",
        "import evaluate\n",
        "\n",
        "# --- spaCy / scispaCy / negspacy Imports (for Clinical Metrics) ---\n",
        "import spacy\n",
        "import scispacy # Required for loading sci models\n",
        "# Optional: for advanced negation detection - uncomment if used\n",
        "# from negspacy.negation import Negex\n",
        "\n",
        "\n",
        "# --- Download NLTK resources ---\n",
        "# Download necessary NLTK data for tokenization and stop words\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except nltk.downloader.DownloadError:\n",
        "    print(\"Downloading NLTK punkt tokenizer...\")\n",
        "    nltk.download('punkt', quiet=True)\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except nltk.downloader.DownloadError:\n",
        "    print(\"Downloading NLTK stopwords...\")\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Download punkt_tab for sentence tokenization if needed (TF-IDF)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt/english.pickle')\n",
        "except nltk.downloader.DownloadError:\n",
        "     print(\"Downloading NLTK punkt_tab for sentence tokenization...\")\n",
        "     nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "\n",
        "# --- Load spaCy and scispaCy model ---\n",
        "# Initialize flags\n",
        "SCISPACY_LOADED = False\n",
        "TARGET_ENTITY_TYPES = set() # Set to empty initially\n",
        "nlp_sci = None # Initialize nlp_sci model\n",
        "\n",
        "print(\"\\n--- Loading scispaCy model for clinical evaluation ---\")\n",
        "print(f\"SpaCy version being used by script: {spacy.__version__}\")\n",
        "try:\n",
        "    from spacy.util import get_data_path\n",
        "    print(f\"SpaCy data path being used by script: {get_data_path()}\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not get spaCy data path: {e}\")\n",
        "\n",
        "try:\n",
        "    # Using the large model for potentially better entity recognition\n",
        "    # Ensure en_core_sci_lg model v0.5.4 is installed and compatible with your spaCy version\n",
        "    nlp_sci = spacy.load(\"en_core_sci_lg\")\n",
        "    # Optional: Add negspacy component if you want to explore negation later\n",
        "    # nlp_sci.add_pipe(\"negex\", config={\"ent_types\":list(TARGET_ENTITY_TYPES)}) # Ensure types match\n",
        "    print(\"scispaCy model 'en_core_sci_lg' loaded successfully.\")\n",
        "    SCISPACY_LOADED = True\n",
        "    # Define target entity types for clinical evaluation (adjust as needed)\n",
        "    TARGET_ENTITY_TYPES = {\"PROBLEM\", \"TREATMENT\", \"TEST\", \"ENTITY\"}\n",
        "    print(f\"Target entity types for clinical evaluation: {TARGET_ENTITY_TYPES}\")\n",
        "\n",
        "except OSError:\n",
        "    print(\"Error: scispaCy model 'en_core_sci_lg' not found or incompatible.\")\n",
        "    print(\"Please ensure it is installed correctly and compatible with your spaCy version.\")\n",
        "    print(\"Refer to scispaCy documentation for installation instructions: https://allenai.github.io/scispacy/\")\n",
        "    SCISPACY_LOADED = False\n",
        "    TARGET_ENTITY_TYPES = set() # Set to empty if model not loaded\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred while loading scispaCy model: {e}\")\n",
        "    SCISPACY_LOADED = False\n",
        "    TARGET_ENTITY_TYPES = set()\n",
        "\n",
        "\n",
        "# --- Evaluation Metric Scorers ---\n",
        "# Initialize ROUGE and BLEU scorers\n",
        "print(\"\\nSetting up ROUGE and BLEU scorers...\")\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "smoother = SmoothingFunction().method1 # For BLEU smoothing\n",
        "\n",
        "# Clean up memory after imports and initial setup\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmdeBI2i9l9N"
      },
      "outputs": [],
      "source": [
        "# (Cell 3) Configuration and Paths\n",
        "\n",
        "# --- Configuration ---\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED) # Seed for random sample selection\n",
        "tqdm.pandas() # Enable progress bars for pandas apply\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "! unzip /content/drive/MyDrive/mimic-iii-10k.zip -d /content/datasetA\n",
        "\n",
        "# --- Paths ---\n",
        "\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/BioBart_Radiology_Summarization\" # Define your base drive path\n",
        "CHECKPOINT_DIR = os.path.join(DRIVE_PATH, \"checkpoints\")\n",
        "METRICS_FILE = os.path.join(DRIVE_PATH, \"training_metrics.json\")\n",
        "FINAL_RESULTS_FILE = os.path.join(DRIVE_PATH, \"final_results.json\")\n",
        "TOKENIZER_PATH = os.path.join(DRIVE_PATH, \"tokenizer\") # Path to save/load custom tokenizer\n",
        "FINAL_MODEL_PATH = os.path.join(DRIVE_PATH, \"complete_model\") # Path to save final model\n",
        "CONFIG_FILE = os.path.join(DRIVE_PATH, \"training_config.json\") # Path for configuration file\n",
        "QUALITATIVE_EXAMPLES_FILE = os.path.join(DRIVE_PATH, \"qualitative_examples.json\") # Path for qualitative examples\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(DRIVE_PATH, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "print(f\"Working directory: {DRIVE_PATH}\")\n",
        "\n",
        "\n",
        "# --- Data Parameters ---\n",
        "DATA_PATH = '/content/datasetA/MIMIC -III (10000 patients)/NOTEEVENTS/NOTEEVENTS_sorted.csv' # Path to your MIMIC-III data\n",
        "SAMPLE_SIZE = 104995 # Number of radiology reports to sample from the full dataset\n",
        "min_word_count = 50 # Minimum word count for an original report to be included\n",
        "max_word_count = 1024 # Maximum word count for the original report (truncate if longer)\n",
        "tfidf_ratio = 0.3 # Ratio of sentences to select for TF-IDF extractive summary\n",
        "\n",
        "\n",
        "# --- Special Tokens Definition ---\n",
        "# Section Headers found in reports and corresponding special tokens\n",
        "SECTION_HEADERS = {\n",
        "    \"INDICATION\": [\"[INDICATION_SEP]\", \"[REASON_SEP]\"],\n",
        "    \"TECHNIQUE\": [\"[TECHNIQUE_SEP]\"],\n",
        "    \"FINDINGS\": [\"[FINDINGS_SEP]\"],\n",
        "    \"IMPRESSION\": [\"[IMPRESSION_SEP]\", \"[CONCLUSION_SEP]\"]\n",
        "}\n",
        "# Regex to find any of the defined headers for parsing\n",
        "ALL_HEADERS_REGEX = \"|\".join([h.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"_SEP\",\"\") for sl in SECTION_HEADERS.values() for h in sl])\n",
        "# List of unique section special tokens\n",
        "SECTION_SPECIAL_TOKENS = list(set([token for sublist in SECTION_HEADERS.values() for token in sublist]))\n",
        "print(f\"Section special tokens defined: {SECTION_SPECIAL_TOKENS}\")\n",
        "\n",
        "# Length Control Tokens based on target summary word count\n",
        "LENGTH_CONTROL_TOKENS = [\"<SUM_SHORT>\", \"<SUM_MEDIUM>\", \"<SUM_LONG>\"]\n",
        "# Thresholds for defining short, medium, long summaries (in words)\n",
        "short_threshold = 50\n",
        "medium_threshold = 100\n",
        "print(f\"Length control tokens defined: {LENGTH_CONTROL_TOKENS}\")\n",
        "\n",
        "# All new tokens to be added to the tokenizer vocabulary\n",
        "ALL_NEW_SPECIAL_TOKENS = list(set(SECTION_SPECIAL_TOKENS + LENGTH_CONTROL_TOKENS))\n",
        "\n",
        "\n",
        "# --- Model and Training Parameters ---\n",
        "model_name = \"GanjinZero/biobart-v2-base\" # Pre-trained BART model\n",
        "MAX_INPUT_LENGTH = 512  # Max token length for model input (includes control + section tokens + text)\n",
        "MAX_TARGET_LENGTH = 150 # Max token length for model output (TF-IDF target summary)\n",
        "\n",
        "INITIAL_LR = 2e-5 # Initial learning rate for the optimizer\n",
        "num_epochs = 15 # Total number of training epochs\n",
        "batch_size = 8 # Batch size per GPU/device\n",
        "gradient_accumulation_steps = 8 # Number of batches to accumulate gradients over\n",
        "effective_batch_size = batch_size * gradient_accumulation_steps\n",
        "print(f\"Effective batch size: {effective_batch_size}\")\n",
        "\n",
        "# Progressive Unfreezing Schedule: Define which layers to unfreeze at which epoch\n",
        "# Note: Epoch numbers are 0-indexed here, but in training logs they are +1 (1-indexed)\n",
        "progressive_unfreezing_schedule = {\n",
        "    3: \"unfreeze_all_decoder\", # Example: Unfreeze all decoder layers at epoch 3\n",
        "    8: \"unfreeze_half_encoder\" # Example: Unfreeze top half of encoder layers at epoch 8\n",
        "    # Add more epochs and layers as needed\n",
        "}\n",
        "# Initial freezing happens after loading the base model and resizing embeddings\n",
        "# Default: Unfreeze shared embeddings, lm_head, and the last N decoder layers\n",
        "num_decoder_layers_to_unfreeze_initial = 4 # Number of decoder layers to unfreeze initially\n",
        "\n",
        "\n",
        "# Curriculum Learning Schedule: Define how the training data size increases\n",
        "curriculum_learning_schedule = {\n",
        "    \"initial_size\": 20000, # Initial number of training samples\n",
        "    \"increment\": 10000, # Number of samples to add per phase\n",
        "    \"increment_every_epochs\": 3 # Number of epochs per phase\n",
        "}\n",
        "\n",
        "\n",
        "# --- Generation Parameters for Validation and Testing ---\n",
        "# These parameters control the beam search during model.generate()\n",
        "generation_parameters = {\n",
        "    \"max_length\": MAX_TARGET_LENGTH + 10, # Maximum length of generated summary\n",
        "    \"num_beams\": 4, # Number of beams for beam search\n",
        "    \"length_penalty\": 1.0, # Encourages longer summaries if > 1.0, shorter if < 1.0\n",
        "    \"early_stopping\": True, # Stop beam search when all beams have generated an EOS token\n",
        "    \"no_repeat_ngram_size\": 3 # Avoids repeating n-grams of this size\n",
        "}\n",
        "\n",
        "\n",
        "# --- Configuration Dictionary (for saving) ---\n",
        "training_config = {\n",
        "    \"seed\": SEED,\n",
        "    \"drive_path\": DRIVE_PATH,\n",
        "    \"data_path\": DATA_PATH,\n",
        "    \"model_name\": model_name,\n",
        "    \"sample_size\": SAMPLE_SIZE,\n",
        "    \"min_word_count\": min_word_count,\n",
        "    \"max_word_count\": max_word_count,\n",
        "    \"tfidf_ratio\": tfidf_ratio,\n",
        "    \"section_headers\": SECTION_HEADERS,\n",
        "    \"length_control_tokens\": LENGTH_CONTROL_TOKENS,\n",
        "    \"length_control_thresholds\": {\"short\": short_threshold, \"medium\": medium_threshold},\n",
        "    \"all_new_special_tokens\": ALL_NEW_SPECIAL_TOKENS,\n",
        "    \"max_input_length\": MAX_INPUT_LENGTH,\n",
        "    \"max_target_length\": MAX_TARGET_LENGTH,\n",
        "    \"initial_lr\": INITIAL_LR,\n",
        "    \"num_epochs\": num_epochs,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "    \"effective_batch_size\": effective_batch_size,\n",
        "    \"initial_unfrozen_decoder_layers\": num_decoder_layers_to_unfreeze_initial,\n",
        "    \"progressive_unfreezing_schedule\": progressive_unfreezing_schedule,\n",
        "    \"curriculum_learning_schedule\": curriculum_learning_schedule,\n",
        "    \"generation_parameters\": generation_parameters,\n",
        "    \"evaluation_entity_types\": list(TARGET_ENTITY_TYPES) if TARGET_ENTITY_TYPES else [], # Ensure serializable\n",
        "    \"scispacy_model\": \"en_core_sci_lg\",\n",
        "    \"scispacy_loaded\": SCISPACY_LOADED # Record if scispaCy was loaded\n",
        "}\n",
        "\n",
        "# Save the configuration\n",
        "print(f\"\\nSaving training configuration to {CONFIG_FILE}...\")\n",
        "try:\n",
        "    with open(CONFIG_FILE, 'w', encoding='utf-8') as f:\n",
        "        json.dump(training_config, f, ensure_ascii=False, indent=4)\n",
        "    print(\"Configuration saved.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving configuration: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwOVRdMt9mQx"
      },
      "outputs": [],
      "source": [
        "# (Cell 4) Utility Functions (Preprocessing Helpers)\n",
        "\n",
        "# Function to add section tokens\n",
        "def add_section_tokens(text, section_headers_map=SECTION_HEADERS, all_headers_regex=ALL_HEADERS_REGEX):\n",
        "    \"\"\"Parses report text and inserts section-specific special tokens.\"\"\"\n",
        "    if not text or not all_headers_regex: return text\n",
        "    processed_text = text\n",
        "    # Regex to find section headers, handling variations and start of line/document\n",
        "    pattern = re.compile(r\"(?:^|\\\\n\\\\s*)(\" + all_headers_regex + r\")\\\\s*:?\\\\s*\", re.IGNORECASE)\n",
        "    last_match_end = 0\n",
        "    modified_parts = []\n",
        "    try:\n",
        "        for match in pattern.finditer(text):\n",
        "            header_found = match.group(1).upper() # Get the matched header text\n",
        "            match_start = match.start(1) # Start position of the matched header text\n",
        "            special_token = None\n",
        "            # Find the corresponding special token for the matched header\n",
        "            for canonical, tokens in section_headers_map.items():\n",
        "                # Create header names from the tokens in the map for matching\n",
        "                headers_in_map = [h.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"_SEP\",\"\") for h in tokens]\n",
        "                if header_found in headers_in_map:\n",
        "                    special_token = tokens[0] # Use the first token in the list as the main one\n",
        "                    break\n",
        "\n",
        "            if special_token:\n",
        "                # Add text before the header, the special token, and a space\n",
        "                modified_parts.append(text[last_match_end:match_start])\n",
        "                modified_parts.append(special_token + \" \")\n",
        "                last_match_end = match.end(0) # Update the end position to the end of the full match (header + colon/space)\n",
        "\n",
        "        # Add the remaining text after the last match\n",
        "        modified_parts.append(text[last_match_end:])\n",
        "\n",
        "        processed_text = \"\".join(modified_parts)\n",
        "        processed_text = ' '.join(processed_text.split()) # Normalize whitespace\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Error during section token insertion - {e}. Returning original text.\")\n",
        "        return text # Return original text on error\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "\n",
        "# Function for TF-IDF Extractive Summary\n",
        "def create_extractive_summary_tfidf(text, ratio=0.3):\n",
        "    \"\"\"Creates an extractive summary using TF-IDF.\"\"\"\n",
        "    if not text or len(text.split()) < min_word_count: # Use same threshold as original data filtering\n",
        "        return text # Return original text if too short\n",
        "\n",
        "    try:\n",
        "        # Use NLTK's punkt for sentence tokenization\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        if len(sentences) <= 3: # Handle very short texts or texts with few sentences\n",
        "            num_sentences = max(1, int(len(sentences) * ratio))\n",
        "            return ' '.join(sentences[:num_sentences])\n",
        "\n",
        "        # Use scikit-learn's default English stop words\n",
        "        tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "        tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
        "\n",
        "        # Calculate sentence scores by summing TF-IDF values\n",
        "        sentence_scores = np.array(tfidf_matrix.sum(axis=1)).flatten()\n",
        "\n",
        "        # Get indices of top sentences based on score\n",
        "        num_sentences = max(1, int(len(sentences) * ratio))\n",
        "        # Use argsort to get indices of sorted scores, take the top ones\n",
        "        top_sentence_indices = sentence_scores.argsort()[-num_sentences:]\n",
        "        # Sort indices to maintain original order of sentences in the summary\n",
        "        top_sentence_indices = sorted(top_sentence_indices)\n",
        "\n",
        "        summary = ' '.join([sentences[i] for i in top_sentence_indices])\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        # Fallback to taking the first few sentences on error\n",
        "        print(f\"Warning: TF-IDF summarization failed - {e}. Returning first sentences.\")\n",
        "        try:\n",
        "             sentences = sent_tokenize(text)\n",
        "             num_sentences = max(1, int(len(sentences) * ratio))\n",
        "             return ' '.join(sentences[:num_sentences])\n",
        "        except:\n",
        "             # Final fallback: return original text if short, else empty\n",
        "             return text if len(text.split()) < min_word_count else \"\"\n",
        "\n",
        "\n",
        "# Function to get length control token based on target summary word count\n",
        "def get_length_control_token(target_summary_text, short_threshold=short_threshold, medium_threshold=medium_threshold):\n",
        "    \"\"\"Determines length control token based on target summary word count.\"\"\"\n",
        "    word_count = len(str(target_summary_text).split()) # Ensure it's a string\n",
        "    if word_count <= short_threshold: return \"<SUM_SHORT>\"\n",
        "    elif word_count <= medium_threshold: return \"<SUM_MEDIUM>\"\n",
        "    else: return \"<SUM_LONG>\"\n",
        "\n",
        "# Text normalization for evaluation\n",
        "def normalize_text(text):\n",
        "    \"\"\"Normalizes text for consistent ROUGE/BLEU evaluation.\"\"\"\n",
        "    if not text: return \"\" # Return empty string for empty input\n",
        "    # Convert to lower case and strip whitespace\n",
        "    text = str(text).lower().strip() # Ensure string type\n",
        "    # Replace multiple spaces with a single space\n",
        "    text = ' '.join(text.split())\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56NsUf759t77"
      },
      "outputs": [],
      "source": [
        "# (Cell 5) Data Loading and Preprocessing\n",
        "\n",
        "print(\"Loading and preprocessing data...\")\n",
        "\n",
        "try:\n",
        "    # Load data from the specified path\n",
        "    data = pd.read_csv(DATA_PATH)\n",
        "    print(f\"Loaded data from {DATA_PATH}\")\n",
        "\n",
        "    # Filter for Radiology reports and create a copy to avoid SettingWithCopyWarning\n",
        "    df = data[data['CATEGORY'] == 'Radiology'].copy()\n",
        "    print(f\"Filtered {len(df)} Radiology reports.\")\n",
        "\n",
        "    # --- Sampling ---\n",
        "    if len(df) > SAMPLE_SIZE:\n",
        "         full_df = df.sample(n=SAMPLE_SIZE, random_state=SEED).reset_index(drop=True)\n",
        "    else:\n",
        "         full_df = df.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "    print(f\"Selected {len(full_df)} Radiology samples for processing.\")\n",
        "\n",
        "    # --- Text Cleaning and Initial Filtering ---\n",
        "    print(\"Cleaning text and applying initial word count filters...\")\n",
        "    full_df['TEXT'] = full_df['TEXT'].fillna(\"\").astype(str) # Ensure string type and handle NaNs\n",
        "    # Normalize whitespace (replace newlines/returns with spaces, then compress spaces)\n",
        "    full_df['TEXT'] = full_df['TEXT'].progress_apply(lambda x: ' '.join(x.replace('\\\\n', ' ').replace('\\\\r', ' ').strip().split()))\n",
        "\n",
        "    # Filter texts shorter than min_word_count\n",
        "    original_len = len(full_df)\n",
        "    full_df = full_df[full_df['TEXT'].apply(lambda x: len(x.split())) >= min_word_count].copy()\n",
        "    print(f\"Removed {original_len - len(full_df)} samples shorter than {min_word_count} words.\")\n",
        "\n",
        "    # Truncate texts longer than max_word_count\n",
        "    full_df['TEXT'] = full_df['TEXT'].progress_apply(lambda x: ' '.join(x.split()[:max_word_count]))\n",
        "    print(f\"Truncated texts longer than {max_word_count} words.\")\n",
        "\n",
        "\n",
        "    # --- Generate TF-IDF Target Summary ---\n",
        "    print(f\"Generating TF-IDF target summaries ('target_text') with ratio {tfidf_ratio}...\")\n",
        "    full_df['target_text'] = full_df['TEXT'].progress_apply(lambda x: create_extractive_summary_tfidf(x, ratio=tfidf_ratio))\n",
        "\n",
        "\n",
        "    # --- Add Section Tokens ---\n",
        "    print(\"Adding section tokens to original text ('section_aware_text')...\")\n",
        "    full_df['section_aware_text'] = full_df['TEXT'].progress_apply(add_section_tokens)\n",
        "\n",
        "\n",
        "    # --- Determine Length Control Token ---\n",
        "    print(f\"Determining length control tokens ('control_token') based on target summary word count (thresholds: short<={short_threshold}, medium<={medium_threshold})...\")\n",
        "    full_df['control_token'] = full_df['target_text'].apply(lambda x: get_length_control_token(x, short_threshold, medium_threshold))\n",
        "\n",
        "\n",
        "    # --- Create Final Input Text ---\n",
        "    print(\"Creating final input text ('input_text') by prepending control token...\")\n",
        "    # The final input text format is <LENGTH_TOKEN> <SECTION_AWARE_TEXT>\n",
        "    full_df['input_text'] = full_df['control_token'] + \" \" + full_df['section_aware_text']\n",
        "\n",
        "\n",
        "    # --- Final Cleanup: Remove samples with empty targets/inputs after processing ---\n",
        "    # This step is crucial if TF-IDF or section parsing resulted in empty strings for some samples\n",
        "    original_len = len(full_df)\n",
        "    full_df = full_df[full_df['target_text'].apply(lambda x: len(str(x).strip()) > 0)].copy() # Ensure string type before strip\n",
        "    full_df = full_df[full_df['input_text'].apply(lambda x: len(str(x).strip()) > 0)].copy()   # Ensure string type before strip\n",
        "    print(f\"Removed {original_len - len(full_df)} samples with empty targets/inputs after processing.\")\n",
        "    full_df = full_df.reset_index(drop=True) # Reset index after filtering\n",
        "\n",
        "\n",
        "    # --- Data Splitting ---\n",
        "    print(\"Splitting data into Train, Validation, and Test sets...\")\n",
        "    # Keep original TEXT column for clinical evaluation raw input\n",
        "    train_df, temp_df = train_test_split(full_df[['input_text', 'target_text', 'TEXT']],\n",
        "                                         test_size=0.3, random_state=SEED)\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=SEED)\n",
        "\n",
        "    # Reset indices for easier access later\n",
        "    train_df = train_df.reset_index(drop=True)\n",
        "    val_df = val_df.reset_index(drop=True)\n",
        "    test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "    print(f\"Train set size: {len(train_df)}\")\n",
        "    print(f\"Validation set size: {len(val_df)}\")\n",
        "    print(f\"Test set size: {len(test_df)}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Data file not found at {DATA_PATH}. Please check the path in the Configuration cell.\")\n",
        "    # Exit or handle gracefully\n",
        "    # exit()\n",
        "except KeyError as e:\n",
        "    print(f\"Error: Missing expected column in CSV: {e}. Ensure 'TEXT' and 'CATEGORY' columns exist in your data file.\")\n",
        "    # exit()\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during data loading/preprocessing: {e}\")\n",
        "    # exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKjyUBpG9yMT"
      },
      "outputs": [],
      "source": [
        "# (Cell 6) Custom Dataset Class\n",
        "\n",
        "class MIMICDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for MIMIC-III Radiology Reports.\"\"\"\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_input_length, max_target_length):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_input_length = max_input_length\n",
        "        self.max_target_length = max_target_length\n",
        "        # Check required columns in the dataframe\n",
        "        required_cols = ['input_text', 'target_text', 'TEXT'] # TEXT holds original for raw_input\n",
        "        if not all(col in dataframe.columns for col in required_cols):\n",
        "             raise ValueError(f\"Dataframe must contain columns: {required_cols}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Retrieves a single sample by index.\"\"\"\n",
        "        if idx >= len(self.dataframe): raise IndexError(\"Index out of bounds\")\n",
        "\n",
        "        # Get the texts for the current sample\n",
        "        input_text = str(self.dataframe.iloc[idx]['input_text'])   # Final input with control+section tokens\n",
        "        target_text = str(self.dataframe.iloc[idx]['target_text']) # TF-IDF summary (Target for loss)\n",
        "        raw_original_text = str(self.dataframe.iloc[idx]['TEXT'])  # Original report text (for clinical metrics)\n",
        "\n",
        "\n",
        "        # Tokenize the input text\n",
        "        input_encoding = self.tokenizer(\n",
        "            input_text,\n",
        "            max_length=self.max_input_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\" # Return PyTorch tensors\n",
        "        )\n",
        "\n",
        "        # Tokenize the target text. Use as_target_tokenizer() for correct handling of BOS/EOS/padding tokens for the decoder.\n",
        "        with self.tokenizer.as_target_tokenizer():\n",
        "            target_encoding = self.tokenizer(\n",
        "                target_text,\n",
        "                max_length=self.max_target_length,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\" # Return PyTorch tensors\n",
        "            )\n",
        "\n",
        "        labels = target_encoding['input_ids']\n",
        "        # Replace padding token id in labels with -100 so it's ignored in the loss calculation\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_encoding['input_ids'].flatten(), # Remove batch dimension\n",
        "            'attention_mask': input_encoding['attention_mask'].flatten(), # Remove batch dimension\n",
        "            'labels': labels.flatten(), # Remove batch dimension\n",
        "            'raw_input': raw_original_text, # Include original text for clinical evaluation\n",
        "            'raw_target': target_text      # Include TF-IDF target for standard evaluation\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgXLCEO590IF"
      },
      "outputs": [],
      "source": [
        "# (Cell 7) Evaluation Metric Functions\n",
        "\n",
        "def calculate_metrics(references, hypotheses):\n",
        "\n",
        "    # Normalize texts before calculating metrics\n",
        "    references = [normalize_text(ref) for ref in references]\n",
        "    hypotheses = [normalize_text(hyp) for hyp in hypotheses]\n",
        "\n",
        "    rouge1_scores, rouge2_scores, rougeL_scores, bleu_scores = [], [], [], []\n",
        "\n",
        "    # Calculate ROUGE scores for each pair\n",
        "    for ref, hyp in zip(references, hypotheses):\n",
        "        # Skip empty references or hypotheses for ROUGE calculation\n",
        "        if not ref or not hyp:\n",
        "             # print(f\"Warning: Skipping empty reference or hypothesis for ROUGE. Ref: '{ref[:20]}...', Hyp: '{hyp[:20]}...'\")\n",
        "            continue\n",
        "        try:\n",
        "            # Calculate ROUGE scores for the current reference-hypothesis pair\n",
        "            scores = scorer.score(ref, hyp)\n",
        "            rouge1_scores.append(scores['rouge1'].fmeasure)\n",
        "            rouge2_scores.append(scores['rouge2'].fmeasure)\n",
        "            rougeL_scores.append(scores['rougeL'].fmeasure)\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating ROUGE for pair: ref='{ref[:50]}...', hyp='{hyp[:50]}...'. Error: {e}\")\n",
        "\n",
        "\n",
        "    # Calculate BLEU scores for each pair\n",
        "    for ref, hyp in zip(references, hypotheses):\n",
        "         # Skip empty references or hypotheses for BLEU calculation\n",
        "         if not ref or not hyp:\n",
        "              # print(f\"Warning: Skipping empty reference or hypothesis for BLEU. Ref: '{ref[:20]}...', Hyp: '{hyp[:20]}...'\")\n",
        "             continue\n",
        "         try:\n",
        "            # Tokenize sentences for BLEU calculation\n",
        "            ref_tokens = nltk.word_tokenize(ref)\n",
        "            hyp_tokens = nltk.word_tokenize(hyp)\n",
        "            # Skip if tokenization results in empty lists\n",
        "            if not ref_tokens or not hyp_tokens:\n",
        "                 # print(f\"Warning: Skipping BLEU due to empty token lists. Ref: '{ref[:20]}...', Hyp: '{hyp[:20]}...'\")\n",
        "                 continue\n",
        "            # Calculate BLEU score using sentence_bleu with smoothing\n",
        "            # sentence_bleu expects a list of reference sentences (even if only one)\n",
        "            bleu_score = sentence_bleu([ref_tokens], hyp_tokens, smoothing_function=smoother)\n",
        "            bleu_scores.append(bleu_score)\n",
        "         except Exception as e:\n",
        "             print(f\"Error calculating BLEU for pair: ref='{ref[:50]}...', hyp='{hyp[:50]}...'. Error: {e}\")\n",
        "\n",
        "\n",
        "    # Calculate average scores, handling cases with no valid scores\n",
        "    avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores) if rouge1_scores else 0.0\n",
        "    avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores) if rouge2_scores else 0.0\n",
        "    avg_rougeL = sum(rougeL_scores) / len(rougeL_scores) if rougeL_scores else 0.0\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0\n",
        "\n",
        "    return {\n",
        "        'rouge-1': avg_rouge1,\n",
        "        'rouge-2': avg_rouge2,\n",
        "        'rouge-l': avg_rougeL,\n",
        "        'bleu': avg_bleu\n",
        "    }\n",
        "\n",
        "\n",
        "# Clinical Metrics (Entity Overlap F1)\n",
        "# scispaCy model (nlp_sci) and TARGET_ENTITY_TYPES are initialized in Cell 1\n",
        "\n",
        "def calculate_clinical_metrics(references_raw, hypotheses, target_entity_types=TARGET_ENTITY_TYPES):\n",
        "\n",
        "    # Check if scispaCy model is loaded and target entity types are defined\n",
        "    if not SCISPACY_LOADED or not target_entity_types:\n",
        "        if not SCISPACY_LOADED:\n",
        "             print(\"Warning: scispaCy model not loaded. Skipping clinical metrics calculation.\")\n",
        "        elif not target_entity_types:\n",
        "             print(\"Warning: No target entity types defined for clinical metrics. Skipping calculation.\")\n",
        "        return {'entity_recall': 0.0, 'entity_precision': 0.0, 'entity_f1': 0.0}\n",
        "\n",
        "    all_recalls, all_precisions, all_f1s = [], [], []\n",
        "    processed_pairs_count = 0 # Counter for pairs successfully processed by scispaCy\n",
        "\n",
        "    print(f\"Calculating clinical metrics for {len(references_raw)} pairs using entity types: {target_entity_types}...\")\n",
        "    # Use tqdm for a progress bar during this potentially slow process\n",
        "    for ref_text, hyp_text in tqdm(zip(references_raw, hypotheses), total=len(references_raw), desc=\"Clinical Metrics\"):\n",
        "        # Ensure texts are strings and not empty or just whitespace\n",
        "        ref_text = str(ref_text).strip()\n",
        "        hyp_text = str(hyp_text).strip()\n",
        "\n",
        "        if not ref_text or not hyp_text:\n",
        "            # print(f\"Skipping pair due to empty reference or hypothesis string after stripping.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Process the original raw text (reference for entities) with scispaCy\n",
        "            doc_ref = nlp_sci(ref_text)\n",
        "            # Extract entities of target types, convert lemma to lower case for comparison\n",
        "            ref_entities = {ent.lemma_.lower() for ent in doc_ref.ents if ent.label_ in target_entity_types}\n",
        "\n",
        "            # Process the generated summary (hypothesis) with scispaCy\n",
        "            doc_hyp = nlp_sci(hyp_text)\n",
        "            hyp_entities = {ent.lemma_.lower() for ent in doc_hyp.ents if ent.label_ in target_entity_types}\n",
        "\n",
        "            # Calculate the number of entities common to both original text and generated summary\n",
        "            common_entities_count = len(ref_entities.intersection(hyp_entities))\n",
        "\n",
        "            # Calculate Recall, Precision, and F1 score for this pair\n",
        "            # Recall: Proportion of entities in the reference (original text) that are found in the hypothesis (generated summary)\n",
        "            recall = common_entities_count / len(ref_entities) if len(ref_entities) > 0 else 0.0\n",
        "            # Precision: Proportion of entities in the hypothesis (generated summary) that are found in the reference (original text)\n",
        "            precision = common_entities_count / len(hyp_entities) if len(hyp_entities) > 0 else 0.0\n",
        "            # F1 Score: Harmonic mean of Precision and Recall\n",
        "            f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "            # Append scores for this pair to the lists\n",
        "            all_recalls.append(recall)\n",
        "            all_precisions.append(precision)\n",
        "            all_f1s.append(f1)\n",
        "            processed_pairs_count += 1 # Increment counter for successfully processed pairs\n",
        "\n",
        "        except Exception as e:\n",
        "            # Catch any errors during scispaCy processing for a specific pair\n",
        "            print(f\"\\nError processing pair with scispaCy (Index {processed_pairs_count}) - ref='{ref_text[:50]}...', hyp='{hyp_text[:50]}...'. Error: {e}\\n\")\n",
        "            # Decide how to handle errors: skipping the pair or appending zeros. Skipping is chosen here.\n",
        "            continue\n",
        "\n",
        "    print(f\"Finished processing {processed_pairs_count} pairs for clinical metrics.\")\n",
        "\n",
        "    # Calculate average scores across all processed pairs, handling case with no processed pairs\n",
        "    avg_recall = sum(all_recalls) / len(all_recalls) if all_recalls else 0.0\n",
        "    avg_precision = sum(all_precisions) / len(all_precisions) if all_precisions else 0.0\n",
        "    avg_f1 = sum(all_f1s) / len(all_f1s) if all_f1s else 0.0\n",
        "\n",
        "    return {\n",
        "        'entity_recall': avg_recall,\n",
        "        'entity_precision': avg_precision,\n",
        "        'entity_f1': avg_f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRbJK56O933r"
      },
      "outputs": [],
      "source": [
        "# (Cell 8) Training Strategy Functions\n",
        "\n",
        "# Curriculum Learning Function\n",
        "def get_curriculum_dataset(current_epoch, train_df, tokenizer,\n",
        "                           initial_size=curriculum_learning_schedule[\"initial_size\"],\n",
        "                           increment=curriculum_learning_schedule[\"increment\"],\n",
        "                           increment_every=curriculum_learning_schedule[\"increment_every_epochs\"]):\n",
        "\n",
        "    # Determine the current phase based on the epoch number\n",
        "    epoch_phase = current_epoch // increment_every\n",
        "\n",
        "    # Calculate the target sample size for the current phase\n",
        "    sample_size = initial_size + epoch_phase * increment\n",
        "\n",
        "    # Ensure the calculated sample size does not exceed the total number of available training samples\n",
        "    sample_size = min(sample_size, len(train_df))\n",
        "\n",
        "    print(f\"Epoch {current_epoch+1}: Using {sample_size} training samples (Curriculum Learning).\")\n",
        "\n",
        "    # Create a subset of the training dataframe using the calculated sample size\n",
        "    # .iloc[:sample_size] takes the first 'sample_size' rows\n",
        "    current_train_df = train_df.iloc[:sample_size]\n",
        "\n",
        "    # Create and return a MIMICDataset instance for the selected subset\n",
        "    return MIMICDataset(current_train_df, tokenizer, max_input_length=MAX_INPUT_LENGTH, max_target_length=MAX_TARGET_LENGTH)\n",
        "\n",
        "\n",
        "# Progressive Unfreezing Function\n",
        "def unfreeze_layers(model, epoch, optimizer, initial_lr, unfreezing_schedule=progressive_unfreezing_schedule):\n",
        "    \"\"\"\n",
        "    Unfreezes more layers of the model at specified epochs and re-initializes optimizer if needed.\n",
        "\n",
        "    Args:\n",
        "        model: The model instance.\n",
        "        epoch (int): The current epoch number (starting from 0).\n",
        "        optimizer: The current optimizer instance.\n",
        "        initial_lr: The initial learning rate.\n",
        "        unfreezing_schedule (dict): Dictionary mapping epoch numbers (0-indexed) to unfreezing actions.\n",
        "\n",
        "    Returns:\n",
        "        The potentially re-initialized optimizer.\n",
        "    \"\"\"\n",
        "    optimizer_reset_needed = False # Flag to indicate if the optimizer needs to be reset\n",
        "\n",
        "    # Check if the current epoch is in the unfreezing schedule\n",
        "    if epoch in unfreezing_schedule:\n",
        "        action = unfreezing_schedule[epoch]\n",
        "        print(f\"\\n--- Epoch {epoch+1}: Applying Progressive Unfreezing action: '{action}' ---\")\n",
        "\n",
        "        if action == \"unfreeze_all_decoder\":\n",
        "            # Unfreeze all parameters in the decoder\n",
        "            for name, param in model.model.decoder.named_parameters():\n",
        "                 if not param.requires_grad: # Only change if not already trainable\n",
        "                      param.requires_grad = True\n",
        "                      optimizer_reset_needed = True\n",
        "            print(\"All decoder layers are now trainable.\")\n",
        "\n",
        "        elif action == \"unfreeze_half_encoder\":\n",
        "            # Unfreeze the top half of the encoder layers\n",
        "            encoder_layers = model.model.encoder.layers\n",
        "            num_encoder_layers = len(encoder_layers)\n",
        "            num_layers_to_unfreeze = num_encoder_layers // 2 # Unfreeze the top half\n",
        "\n",
        "            print(f\"Unfreezing the top {num_layers_to_unfreeze} out of {num_encoder_layers} encoder layers.\")\n",
        "            # Iterate through encoder layers and unfreeze the top ones\n",
        "            for i, layer in enumerate(encoder_layers):\n",
        "                 # Unfreeze layers from num_layers_to_unfreeze onwards (0-indexed)\n",
        "                 if i >= (num_encoder_layers - num_layers_to_unfreeze):\n",
        "                      # Check if any parameter in the layer is not already trainable\n",
        "                      if not any(p.requires_grad for p in layer.parameters()):\n",
        "                           print(f\"Unfreezing encoder layer {i} (Index {i}/{num_encoder_layers-1})\")\n",
        "                           for param in layer.parameters():\n",
        "                                param.requires_grad = True\n",
        "                           optimizer_reset_needed = True\n",
        "            print(\"Specified encoder layers are now trainable.\")\n",
        "\n",
        "        else:\n",
        "            print(f\"Warning: Unknown unfreezing action '{action}' specified in schedule for epoch {epoch+1}.\")\n",
        "\n",
        "\n",
        "    # This ensures the new parameters are added to the optimizer's state.\n",
        "    if optimizer_reset_needed:\n",
        "         print(\"Re-initializing optimizer due to layer unfreezing...\")\n",
        "         # Create a new optimizer instance, including all currently trainable parameters\n",
        "         trainable_model_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "         optimizer = AdamW(trainable_model_params, lr=initial_lr) # Use the initial learning rate\n",
        "         print(\"Optimizer re-initialized with current trainable parameters.\")\n",
        "\n",
        "    # Print the ratio of trainable parameters after applying any unfreezing\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Current trainable parameters: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\\\\n\")\n",
        "\n",
        "    return optimizer # Return the (potentially new) optimizer instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfqYOovn97Kr"
      },
      "outputs": [],
      "source": [
        "# (Cell 9) Checkpointing and Metrics Saving Functions\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, metrics, checkpoint_path):\n",
        "\n",
        "    print(f\"Saving checkpoint for epoch {epoch+1} to {checkpoint_path}...\")\n",
        "    try:\n",
        "        checkpoint = {\n",
        "            'epoch': epoch, # Save the 0-indexed epoch number\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'metrics': metrics # Save the latest validation metrics dictionary\n",
        "        }\n",
        "        # Ensure directory exists before saving\n",
        "        os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "        print(\"Checkpoint saved successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving checkpoint to {checkpoint_path}: {e}\")\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer, device):\n",
        "\n",
        "    # Check if the checkpoint file exists\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
        "        try:\n",
        "            # Load the checkpoint dictionary, mapping to the specified device\n",
        "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "            # Load the model state dictionary\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            print(\"Model state loaded.\")\n",
        "\n",
        "            # Load the optimizer state dictionary only if an optimizer is provided\n",
        "            if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n",
        "                 try:\n",
        "                      # Load optimizer state - handles parameters added by unfreezing if they match\n",
        "                      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "                      print(\"Optimizer state loaded.\")\n",
        "                 except ValueError as e:\n",
        "                      # This can happen if the set of trainable parameters changed significantly\n",
        "                      print(f\"Warning: Could not load optimizer state, possibly due to parameter changes: {e}\")\n",
        "                      print(\"Optimizer state will be re-initialized for trainable parameters.\")\n",
        "                      # Re-initialize optimizer with current trainable parameters if loading failed\n",
        "                      if optimizer is not None: # Double check optimizer is not None\n",
        "                           trainable_model_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "                           # Use the LR from the loaded checkpoint if available, otherwise fallback to initial\n",
        "                           loaded_lr = next(iter(checkpoint['optimizer_state_dict']['param_groups']))['lr'] if 'optimizer_state_dict' in checkpoint and checkpoint['optimizer_state_dict']['param_groups'] else INITIAL_LR\n",
        "                           optimizer = AdamW(trainable_model_params, lr=loaded_lr)\n",
        "                           print(\"Optimizer re-initialized with current trainable parameters.\")\n",
        "\n",
        "\n",
        "            # Determine the epoch to start from. Add 1 because the saved epoch is the one that just finished.\n",
        "            # Use .get with a default for robustness against older checkpoint formats\n",
        "            start_epoch = checkpoint.get('epoch', -1) + 1\n",
        "            # Load the metrics dictionary saved with this checkpoint\n",
        "            loaded_metrics = checkpoint.get('metrics', {})\n",
        "\n",
        "            print(f\"Checkpoint loaded successfully. Resuming training from epoch {start_epoch}\")\n",
        "            return model, optimizer, start_epoch, loaded_metrics\n",
        "\n",
        "        except Exception as e:\n",
        "            # Handle errors during loading\n",
        "            print(f\"Error loading checkpoint from {checkpoint_path}: {e}\")\n",
        "            print(\"Starting training from scratch (Epoch 0).\")\n",
        "            # Return initial state if loading fails\n",
        "            return model, optimizer, 0, {} # Start from epoch 0, return empty metrics\n",
        "\n",
        "    else:\n",
        "        # If checkpoint file does not exist\n",
        "        print(f\"Checkpoint file not found at {checkpoint_path}. Starting training from scratch (Epoch 0).\")\n",
        "        return model, optimizer, 0, {} # Start from epoch 0, return empty metrics\n",
        "\n",
        "\n",
        "def save_metrics(metrics_dict, file_path):\n",
        "\n",
        "    print(f\"Saving metrics history to {file_path}...\")\n",
        "    try:\n",
        "        # Ensure directory exists\n",
        "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "        with open(file_path, 'w', encoding='utf-8') as f:\n",
        "            # Convert any numpy types (like numpy.float64) to native Python types\n",
        "            # for JSON serialization. This is done for lists of numbers.\n",
        "            serializable_metrics = {}\n",
        "            for key, value in metrics_dict.items():\n",
        "                if isinstance(value, list) and value and isinstance(value[0], (np.generic, float, int)):\n",
        "                     # Convert each item in the list if it's a numpy generic type\n",
        "                     serializable_metrics[key] = [item.item() if isinstance(item, np.generic) else item for item in value]\n",
        "                else:\n",
        "                     # For non-list values or lists of other types, just use the value directly\n",
        "                     serializable_metrics[key] = value\n",
        "\n",
        "            # Dump the serializable dictionary to the JSON file with indentation\n",
        "            json.dump(serializable_metrics, f, ensure_ascii=False, indent=4)\n",
        "        print(\"Metrics history saved successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving metrics history to {file_path}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRMSbNdh99KF"
      },
      "outputs": [],
      "source": [
        "# (Cell 10) Model and Tokenizer Setup\n",
        "\n",
        "print(f\"Loading base model and tokenizer: {model_name}\")\n",
        "try:\n",
        "    # Load base tokenizer and model from Hugging Face Hub\n",
        "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "    # Add ALL new special tokens defined in Configuration cell\n",
        "    print(f\"Adding {len(ALL_NEW_SPECIAL_TOKENS)} new special tokens to tokenizer vocabulary: {ALL_NEW_SPECIAL_TOKENS}\")\n",
        "    special_tokens_dict = {'additional_special_tokens': ALL_NEW_SPECIAL_TOKENS}\n",
        "    num_added = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    print(f\"Number of tokens added: {num_added}\")\n",
        "\n",
        "    # Resize model embeddings to match the new tokenizer size.\n",
        "    # This adds new embedding vectors for the newly added tokens, initialized randomly.\n",
        "    print(f\"Resizing model token embeddings from {model.config.vocab_size} to {len(tokenizer)}\")\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    print(f\"Model vocabulary size after resizing: {model.get_input_embeddings().weight.shape[0]}\")\n",
        "\n",
        "    # Save the modified tokenizer configuration (including new tokens)\n",
        "    tokenizer.save_pretrained(TOKENIZER_PATH)\n",
        "    print(f\"Tokenizer with special tokens saved to {TOKENIZER_PATH}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model/tokenizer or adding tokens: {e}\")\n",
        "    # Exit gracefully if model/tokenizer setup fails\n",
        "    # exit()\n",
        "\n",
        "\n",
        "# --- Initial Layer Freezing Strategy ---\n",
        "# Freeze most layers initially and only train specific parts.\n",
        "print(\"Applying initial layer freezing strategy...\")\n",
        "\n",
        "# Freeze all model parameters by default\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze shared embeddings\n",
        "if model.model.shared is not None:\n",
        "     for param in model.model.shared.parameters():\n",
        "          param.requires_grad = True\n",
        "     print(\"Unfroze shared embeddings.\")\n",
        "\n",
        "# Unfreeze the language model head\n",
        "for param in model.lm_head.parameters():\n",
        "    param.requires_grad = True\n",
        "print(\"Unfroze language model head (lm_head).\")\n",
        "\n",
        "# Unfreeze the last N decoder layers (N is num_decoder_layers_to_unfreeze_initial)\n",
        "decoder_layers = model.model.decoder.layers\n",
        "num_decoder_layers = len(decoder_layers)\n",
        "num_layers_to_unfreeze = min(num_decoder_layers_to_unfreeze_initial, num_decoder_layers) # Ensure N is not more than total layers\n",
        "\n",
        "print(f\"Unfreezing the last {num_layers_to_unfreeze} decoder layers.\")\n",
        "for i in range(num_decoder_layers - num_layers_to_unfreeze, num_decoder_layers):\n",
        "    print(f\"Unfreezing decoder layer {i} (Index {i}/{num_decoder_layers-1})\")\n",
        "    for param in decoder_layers[i].parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "# Print the ratio of trainable parameters after initial freezing\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\nTrainable parameters after initial freezing: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\\\\n\")\n",
        "\n",
        "# Clean up memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4mc025e9_Zr"
      },
      "outputs": [],
      "source": [
        "# (Cell 11) Create Datasets and DataLoaders\n",
        "\n",
        "print(\"Creating datasets and dataloaders...\")\n",
        "\n",
        "try:\n",
        "    # Create Dataset instances for validation and test sets\n",
        "    # Training dataset is created dynamically in the training loop for Curriculum Learning\n",
        "    val_dataset = MIMICDataset(val_df, tokenizer, max_input_length=MAX_INPUT_LENGTH, max_target_length=MAX_TARGET_LENGTH)\n",
        "    test_dataset = MIMICDataset(test_df, tokenizer, max_input_length=MAX_INPUT_LENGTH, max_target_length=MAX_TARGET_LENGTH)\n",
        "\n",
        "    # Create DataLoader instances for validation and test sets\n",
        "    # num_workers > 0 can speed up data loading, pin_memory=True can speed up CPU-to-GPU transfer\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    print(f\"Validation Dataloader created with {len(val_dataloader)} batches.\")\n",
        "    print(f\"Test Dataloader created with {len(test_dataloader)} batches.\")\n",
        "\n",
        "except ValueError as e:\n",
        "     print(f\"Error creating dataset: {e}\")\n",
        "     # exit()\n",
        "except Exception as e:\n",
        "     print(f\"An unexpected error occurred during dataset/dataloader creation: {e}\")\n",
        "     # exit()\n",
        "\n",
        "# --- Device Setup ---\n",
        "# Determine the device to use (GPU if available, otherwise CPU)\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache() # Clear GPU cache before moving model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move the model to the selected device\n",
        "try:\n",
        "    model.to(device)\n",
        "    print(\"Model moved to device.\")\n",
        "except RuntimeError as e:\n",
        "    print(f\"Error moving model to {device}: {e}. Trying CPU.\")\n",
        "    device = torch.device('cpu')\n",
        "    model.to(device)\n",
        "    print(\"Model moved to CPU.\")\n",
        "\n",
        "\n",
        "# --- Optimizer and Scaler Setup ---\n",
        "# Initialize the optimizer with only the trainable parameters\n",
        "# Using AdamW, a common choice for transformer fine-tuning\n",
        "# The set of trainable parameters might change during progressive unfreezing\n",
        "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=INITIAL_LR)\n",
        "print(f\"Optimizer initialized with learning rate: {INITIAL_LR}\")\n",
        "\n",
        "# Initialize GradScaler for Mixed Precision Training (AMP)\n",
        "# This helps speed up training and reduce memory usage on compatible GPUs\n",
        "scaler = GradScaler()\n",
        "print(\"Initialized GradScaler for Automatic Mixed Precision (AMP).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQ0GXLMR-QBL"
      },
      "outputs": [],
      "source": [
        "# (Cell 12) Training Loop\n",
        "\n",
        "print(\"\\nStarting training process...\")\n",
        "\n",
        "# Initialize variables for tracking metrics and best model\n",
        "best_val_metric = 0.0 # Initialize best validation metric (tracking ROUGE-L)\n",
        "best_model_epoch = -1 # Track the epoch number where the best model was found\n",
        "\n",
        "# --- Checkpoint Loading and History Loading ---\n",
        "\n",
        "# Define the metrics keys that are expected in the history file\n",
        "# This list is comprehensive and includes all metrics we track\n",
        "expected_keys = {\n",
        "    'train_loss': [], 'val_loss': [],\n",
        "    'rouge-1': [], 'rouge-2': [], 'rouge-l': [], 'bleu': [],\n",
        "    'entity_recall': [], 'entity_precision': [], 'entity_f1': [],\n",
        "    'sample_sizes': [], # Size of the training dataset subset used in each epoch\n",
        "    'epoch_runtime_seconds': [] # Duration of each epoch in seconds\n",
        "}\n",
        "\n",
        "# Initialize the dictionary to store the training history for all metrics\n",
        "# This dictionary will be populated from a saved file or start fresh\n",
        "all_metrics = {key: [] for key in expected_keys}\n",
        "\n",
        "# Paths for checkpoints\n",
        "latest_model_path = os.path.join(DRIVE_PATH, \"latest_model.pt\")\n",
        "best_model_path = os.path.join(DRIVE_PATH, \"best_model.pt\")\n",
        "\n",
        "# Determine the starting epoch and load model/optimizer state from checkpoint\n",
        "start_epoch = 0 # Default start epoch\n",
        "loaded_metrics_from_checkpoint = {} # Metrics dictionary saved within the checkpoint\n",
        "\n",
        "# If the best model checkpoint is not found, try loading the latest model checkpoint\n",
        "if os.path.exists(latest_model_path):\n",
        "    print(f\"--- Best model not found, loading latest model from {latest_model_path} to resume training ---\")\n",
        "    # load_checkpoint updates model, optimizer, start_epoch, and returns metrics saved with it\n",
        "    model, optimizer, start_epoch, loaded_metrics_from_checkpoint = load_checkpoint(latest_model_path, model, optimizer, device)\n",
        "    # Update best_val_metric based on the latest model's metrics (if available)\n",
        "    best_val_metric = loaded_metrics_from_checkpoint.get('rouge-l', 0.0)\n",
        "    # Get the epoch the latest model was trained at, default to start_epoch - 1\n",
        "    best_model_epoch = loaded_metrics_from_checkpoint.get('epoch', start_epoch - 1)\n",
        "    print(f\"Loaded latest model (Epoch {start_epoch})\") # Note: start_epoch is the *next* epoch number\n",
        "    print(f\"Resuming training from epoch {start_epoch}\")\n",
        "\n",
        "# Prioritize loading the best model checkpoint if it exists\n",
        "elif os.path.exists(best_model_path):\n",
        "    print(f\"--- Loading best model from {best_model_path} to resume training ---\")\n",
        "    # load_checkpoint updates model, optimizer, start_epoch, and returns metrics saved with it\n",
        "    model, optimizer, start_epoch, loaded_metrics_from_checkpoint = load_checkpoint(best_model_path, model, optimizer, device)\n",
        "    # Update best_val_metric and best_model_epoch based on the loaded best model's metrics\n",
        "    best_val_metric = loaded_metrics_from_checkpoint.get('rouge-l', 0.0)\n",
        "    # Get the epoch the best model was trained at, default to start_epoch - 1 if not found\n",
        "    best_model_epoch = loaded_metrics_from_checkpoint.get('epoch', start_epoch - 1)\n",
        "    print(f\"Loaded best model (Epoch {best_model_epoch + 1}) with ROUGE-L: {best_val_metric:.4f}\")\n",
        "    print(f\"Resuming training from epoch {start_epoch}\")\n",
        "\n",
        "# If no checkpoint is found, training starts from scratch (Epoch 0)\n",
        "else:\n",
        "    print(\"--- No checkpoint found, starting training from scratch (Epoch 0) ---\")\n",
        "    start_epoch = 0 # Explicitly set start_epoch to 0\n",
        "\n",
        "\n",
        "# --- Load Metrics History from File ---\n",
        "# Load the full training history (metrics for all previous epochs) from the JSON file\n",
        "# This is done *after* determining start_epoch from checkpoint\n",
        "METRICS_FILE = os.path.join(DRIVE_PATH, \"training_metrics.json\") # Ensure path is defined\n",
        "\n",
        "if os.path.exists(METRICS_FILE):\n",
        "    print(f\"Attempting to load previous metrics history from {METRICS_FILE}\")\n",
        "    try:\n",
        "        with open(METRICS_FILE, 'r', encoding='utf-8') as f:\n",
        "            loaded_metrics_hist = json.load(f)\n",
        "\n",
        "        # Validate the structure and load history up to the start_epoch\n",
        "        if isinstance(loaded_metrics_hist, dict):\n",
        "            # Create a temporary dictionary with the expected keys and empty lists\n",
        "            temp_metrics_for_load = {key: [] for key in expected_keys}\n",
        "            valid_load_successful = True # Flag to track if loading was entirely successful\n",
        "\n",
        "            # Determine the length of history available in the file based on the first expected key\n",
        "            first_key = list(expected_keys.keys())[0]\n",
        "            if first_key in loaded_metrics_hist and isinstance(loaded_metrics_hist[first_key], list):\n",
        "                 file_history_len = len(loaded_metrics_hist[first_key])\n",
        "            else:\n",
        "                 # If the first key is missing or not a list, the file structure is likely invalid\n",
        "                 file_history_len = 0\n",
        "                 valid_load_successful = False\n",
        "                 print(f\"Warning: Metrics file {METRICS_FILE} has unexpected format or is empty.\")\n",
        "\n",
        "\n",
        "            if valid_load_successful:\n",
        "                # Calculate the actual number of epochs to load history for.\n",
        "                # This is the minimum of the determined start_epoch and the history length in the file.\n",
        "                effective_history_len_to_load = min(start_epoch, file_history_len)\n",
        "\n",
        "                if effective_history_len_to_load > 0:\n",
        "                    print(f\"Loading history for {effective_history_len_to_load} previous epochs.\")\n",
        "                    # Load data for each expected key up to the effective history length\n",
        "                    for key in expected_keys:\n",
        "                        if key in loaded_metrics_hist and isinstance(loaded_metrics_hist[key], list) and len(loaded_metrics_hist[key]) >= effective_history_len_to_load:\n",
        "                             # Slice the list to get only the data up to start_epoch (0-indexed)\n",
        "                             temp_metrics_for_load[key] = loaded_metrics_hist[key][:effective_history_len_to_load]\n",
        "                        else:\n",
        "                             # If a key is missing in the file or its list is shorter than needed,\n",
        "                             # print a warning and keep the corresponding list empty in temp_metrics_for_load.\n",
        "                             print(f\"Warning: Data for key '{key}' is missing or incomplete in metrics file for loading up to epoch {start_epoch}. History for this key might be reset.\")\n",
        "                             valid_load_successful = False # Mark load as not fully successful for all keys\n",
        "                             temp_metrics_for_load[key] = [] # Ensure list is empty if load failed for this key\n",
        "\n",
        "                    if valid_load_successful: # If history for all expected keys was loaded correctly\n",
        "                         all_metrics = temp_metrics_for_load # Replace the initial empty all_metrics with loaded history\n",
        "                         print(\"Metrics history loaded successfully and aligned with checkpoint.\")\n",
        "                    else:\n",
        "                         print(\"Issues found during metrics history loading. Starting history fresh to avoid corruption.\")\n",
        "                         # If loading issues occurred, reset all_metrics to empty lists\n",
        "                         all_metrics = {key: [] for key in expected_keys}\n",
        "\n",
        "                # Handle cases where the file history length is greater than start_epoch\n",
        "                # This indicates a potential inconsistency between checkpoint and metrics file\n",
        "                elif file_history_len > start_epoch:\n",
        "                     print(f\"Warning: Metrics file contains data for {file_history_len} epochs, but resuming from epoch {start_epoch}. This suggests inconsistency. Starting history fresh.\")\n",
        "                     all_metrics = {key: [] for key in expected_keys} # Start fresh due to inconsistency\n",
        "\n",
        "                else: # file_history_len <= start_epoch and effective_history_len_to_load == 0\n",
        "                    # This happens if start_epoch is 0 or the file history is very short/empty\n",
        "                    print(\"No previous history needs to be loaded (start_epoch is 0 or history file is empty/short).\")\n",
        "\n",
        "            else: # valid_load_successful == False due to initial file format check failure\n",
        "                 print(\"Metrics file has an invalid initial structure. Starting history fresh.\")\n",
        "                 all_metrics = {key: [] for key in expected_keys} # Start fresh\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from {METRICS_FILE}. File might be corrupt. Starting history fresh.\")\n",
        "        all_metrics = {key: [] for key in expected_keys} # Start fresh\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred loading metrics history: {e}. Starting history fresh.\")\n",
        "        all_metrics = {key: [] for key in expected_keys} # Start fresh\n",
        "\n",
        "else:\n",
        "    print(\"Metrics history file not found. Starting history fresh.\")\n",
        "    # If the file does not exist, all_metrics remains the initial dictionary with empty lists.\n",
        "\n",
        "\n",
        "# --- Main Training and Validation Loop ---\n",
        "# Define the interval for performing expensive evaluations (Clinical)\n",
        "evaluation_interval = 3 # Perform full evaluation every 3 epochs\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    print(f\"\\n===== Epoch {epoch+1}/{num_epochs} =====\")\n",
        "    epoch_start_time = time.time() # Record the start time of the current epoch\n",
        "\n",
        "    # --- Progressive Unfreezing ---\n",
        "    # Apply the unfreezing strategy based on the current epoch number\n",
        "    # This function returns the (potentially new) optimizer instance\n",
        "    optimizer = unfreeze_layers(model, epoch, optimizer, INITIAL_LR, progressive_unfreezing_schedule)\n",
        "\n",
        "    # --- Curriculum Learning ---\n",
        "    # Get the subset of the training data for the current epoch\n",
        "    current_train_dataset = get_curriculum_dataset(epoch, train_df, tokenizer,\n",
        "                                                   initial_size=curriculum_learning_schedule[\"initial_size\"],\n",
        "                                                   increment=curriculum_learning_schedule[\"increment\"],\n",
        "                                                   increment_every=curriculum_learning_schedule[\"increment_every_epochs\"])\n",
        "    # Create the DataLoader for the current training dataset subset\n",
        "    # Use more workers/pin_memory if resources allow for faster data loading\n",
        "    train_dataloader = DataLoader(current_train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # Store the size of the training sample used in this epoch\n",
        "    # This is always recorded\n",
        "    all_metrics['sample_sizes'].append(len(current_train_dataset))\n",
        "\n",
        "\n",
        "    # --- Training Phase ---\n",
        "    model.train() # Set the model to training mode\n",
        "    total_train_loss = 0 # Variable to accumulate training loss over the epoch\n",
        "    optimizer.zero_grad() # Zero the gradients at the beginning of the epoch or accumulation step\n",
        "\n",
        "    # Use tqdm for a progress bar over the training batches\n",
        "    progress_bar_train = tqdm(train_dataloader, desc=f\"Epoch {epoch+1} Training\", leave=False)\n",
        "    for idx, batch in enumerate(progress_bar_train):\n",
        "        # Move batch data to the appropriate device (GPU/CPU)\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # --- Forward Pass with Mixed Precision (AMP) ---\n",
        "        # autocast enables automatic mixed precision, which speeds up training\n",
        "        with autocast(): # Note: Update to torch.amp.autocast('cuda', ...) for newer PyTorch\n",
        "            # Perform the forward pass, calculate loss\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels # Labels are used internally to calculate the loss\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            # Scale the loss by the gradient accumulation steps\n",
        "            # This is necessary because gradients are summed over multiple batches\n",
        "            loss = loss / gradient_accumulation_steps\n",
        "\n",
        "        # --- Backward Pass with GradScaler ---\n",
        "        # Scale the loss before backward pass in AMP\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # --- Optimizer Step (with Gradient Accumulation) ---\n",
        "        # Perform optimizer step only after accumulating gradients over several batches\n",
        "        # Or perform a step for the last batch even if it's less than accumulation steps\n",
        "        if (idx + 1) % gradient_accumulation_steps == 0 or (idx + 1) == len(train_dataloader):\n",
        "            # Optional: Gradient clipping to prevent exploding gradients\n",
        "            # scaler.unscale_(optimizer) # Unscale gradients before clipping\n",
        "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            # Perform optimizer step using the scaled gradients\n",
        "            scaler.step(optimizer)\n",
        "            # Update the scale for the next iteration\n",
        "            scaler.update()\n",
        "            # Zero the gradients after the optimizer step\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # Accumulate the loss (undo the scaling by accumulation steps for accurate logging)\n",
        "        total_train_loss += loss.item() * gradient_accumulation_steps\n",
        "        # Update the progress bar with the current loss\n",
        "        progress_bar_train.set_postfix({'loss': loss.item() * gradient_accumulation_steps})\n",
        "\n",
        "        # Clean up batch variables and clear GPU cache to save memory\n",
        "        del input_ids, attention_mask, labels, outputs, loss\n",
        "        if device == torch.device('cuda'):\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect() # Collect garbage\n",
        "\n",
        "    # Calculate the average training loss for the epoch\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader) # Divide by number of batches\n",
        "    print(f\"Epoch {epoch+1} Average Training Loss: {avg_train_loss:.4f}\")\n",
        "    # Store the average training loss in the metrics history\n",
        "    # This is always recorded\n",
        "    all_metrics['train_loss'].append(avg_train_loss)\n",
        "\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "    total_val_loss = 0 # Variable to accumulate validation loss\n",
        "    # Lists to store generated summaries, references, and raw inputs for evaluation\n",
        "    val_references = [] # TF-IDF targets for ROUGE/BLEU\n",
        "    val_hypotheses = [] # Model generated summaries\n",
        "    val_raw_inputs = [] # Original texts for Clinical (Entity) metrics\n",
        "\n",
        "    print(f\"\\nRunning Validation for Epoch {epoch+1}...\")\n",
        "    # Use tqdm for a progress bar over the validation batches\n",
        "    progress_bar_val = tqdm(val_dataloader, desc=f\"Epoch {epoch+1} Validation\", leave=False)\n",
        "\n",
        "    # Disable gradient calculation during validation to save memory and speed up\n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar_val:\n",
        "            # Move batch data to the appropriate device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            # Get raw texts from the batch (these are not tensors)\n",
        "            raw_input_texts = batch['raw_input']\n",
        "            raw_target_texts = batch['raw_target']\n",
        "\n",
        "            # --- Calculate Validation Loss ---\n",
        "            # Use mixed precision for validation loss calculation as well\n",
        "            with autocast(): # Note: Update to torch.amp.autocast('cuda', ...) for newer PyTorch\n",
        "                 # Perform forward pass to calculate loss\n",
        "                 outputs = model(\n",
        "                     input_ids=input_ids,\n",
        "                     attention_mask=attention_mask,\n",
        "                     labels=labels\n",
        "                 )\n",
        "                 loss = outputs.loss\n",
        "            # Accumulate the validation loss\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "\n",
        "            # --- Generate Summaries ---\n",
        "            # Generate summaries from the input texts using beam search (or other decoding strategies)\n",
        "            # This is done for all validation batches to calculate standard metrics\n",
        "            # Use autocast here if generation benefits from mixed precision (check performance)\n",
        "            # with autocast():\n",
        "            generated_ids = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                # Use generation parameters defined in Configuration cell\n",
        "                max_length=generation_parameters[\"max_length\"],\n",
        "                num_beams=generation_parameters[\"num_beams\"],\n",
        "                length_penalty=generation_parameters[\"length_penalty\"],\n",
        "                early_stopping=generation_parameters[\"early_stopping\"],\n",
        "                no_repeat_ngram_size=generation_parameters[\"no_repeat_ngram_size\"]\n",
        "            )\n",
        "\n",
        "            # Decode the generated token IDs back into human-readable text\n",
        "            hypotheses_batch = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "            # Store the generated summaries, raw targets, and raw inputs for metric calculation later\n",
        "            val_hypotheses.extend(hypotheses_batch)\n",
        "            val_references.extend(raw_target_texts) # TF-IDF targets are references for standard metrics\n",
        "            val_raw_inputs.extend(raw_input_texts)  # Original inputs are references for clinical metrics\n",
        "\n",
        "\n",
        "            # Clean up batch variables and clear GPU cache\n",
        "            del input_ids, attention_mask, labels, raw_input_texts, raw_target_texts, outputs, loss, generated_ids\n",
        "            if device == torch.device('cuda'):\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect() # Collect garbage\n",
        "\n",
        "\n",
        "    # Calculate the average validation loss for the epoch\n",
        "    avg_val_loss = total_val_loss / len(val_dataloader) # Divide by number of batches\n",
        "    print(f\"Epoch {epoch+1} Average Validation Loss: {avg_val_loss:.4f}\")\n",
        "    # Store the average validation loss in the metrics history\n",
        "    # This is always recorded\n",
        "    all_metrics['val_loss'].append(avg_val_loss)\n",
        "\n",
        "\n",
        "    # --- Calculate Standard Validation Metrics (Always) ---\n",
        "    print(\"\\nCalculating Standard Validation Metrics (ROUGE, BLEU)...\")\n",
        "    # Calculate standard metrics comparing generated summaries to TF-IDF targets\n",
        "    metrics_std = calculate_metrics(val_references, val_hypotheses)\n",
        "    print(f\"Std Metrics - R1: {metrics_std['rouge-1']:.4f}, R2: {metrics_std['rouge-2']:.4f}, RL: {metrics_std['rouge-l']:.4f}, B: {metrics_std['bleu']:.4f}\")\n",
        "\n",
        "    # Store Standard Metrics (Always)\n",
        "    all_metrics['rouge-1'].append(metrics_std['rouge-1'])\n",
        "    all_metrics['rouge-2'].append(metrics_std['rouge-2'])\n",
        "    all_metrics['rouge-l'].append(metrics_std['rouge-l'])\n",
        "    all_metrics['bleu'].append(metrics_std['bleu'])\n",
        "\n",
        "\n",
        "    # --- Conditional Calculation of Clinical Metrics ---\n",
        "    # Check if the current epoch is an interval where we perform the full evaluation\n",
        "    if (epoch + 1) % evaluation_interval == 0:\n",
        "        print(f\"\\nEpoch {epoch+1}: Performing full evaluation (Clinical)...\")\n",
        "\n",
        "        # Calculate Clinical Metrics (Entity Overlap)\n",
        "        print(\"Calculating Clinical Validation Metrics (Entity Overlap)...\")\n",
        "        metrics_clin = calculate_clinical_metrics(val_raw_inputs, val_hypotheses)\n",
        "        print(f\"Clinical Metrics - ER: {metrics_clin['entity_recall']:.4f}, EP: {metrics_clin['entity_precision']:.4f}, EF1: {metrics_clin['entity_f1']:.4f}\")\n",
        "\n",
        "        # Store Clinical Metrics for this epoch\n",
        "        all_metrics['entity_recall'].append(metrics_clin['entity_recall'])\n",
        "        all_metrics['entity_precision'].append(metrics_clin['entity_precision'])\n",
        "        all_metrics['entity_f1'].append(metrics_clin['entity_f1'])\n",
        "\n",
        "    else:\n",
        "        # If this is not an evaluation interval epoch, append placeholder values\n",
        "        print(f\"\\nEpoch {epoch+1}: Skipping full evaluation (Clinical). Appending placeholder values.\")\n",
        "        all_metrics['entity_recall'].append(0.0) # Or use None, but 0.0 might be simpler for plotting\n",
        "        all_metrics['entity_precision'].append(0.0)\n",
        "        all_metrics['entity_f1'].append(0.0)\n",
        "\n",
        "\n",
        "    # --- Record Epoch Runtime ---\n",
        "    epoch_end_time = time.time() # Record the end time of the current epoch\n",
        "    epoch_duration = epoch_end_time - epoch_start_time # Calculate epoch duration\n",
        "    all_metrics['epoch_runtime_seconds'].append(epoch_duration) # Store duration in history\n",
        "    print(f\"Epoch {epoch+1} took {epoch_duration:.2f} seconds ({epoch_duration/60:.2f} minutes).\")\n",
        "\n",
        "\n",
        "    # --- Checkpoint Saving ---\n",
        "    # Combine current epoch metrics for saving with the checkpoint (only include computed metrics)\n",
        "    # If conditional evaluation happened, include all metrics; otherwise, only standard + loss\n",
        "    if (epoch + 1) % evaluation_interval == 0:\n",
        "         current_epoch_metrics = {**metrics_std, **metrics_clin,\n",
        "                                  'train_loss': avg_train_loss, 'val_loss': avg_val_loss, 'epoch': epoch}\n",
        "    else:\n",
        "         current_epoch_metrics = {**metrics_std,\n",
        "                                  'train_loss': avg_train_loss, 'val_loss': avg_val_loss, 'epoch': epoch,\n",
        "                                  'entity_recall': 0.0, 'entity_precision': 0.0, 'entity_f1': 0.0} # Add clinical placeholders too\n",
        "\n",
        "\n",
        "    # Save the latest model checkpoint after each epoch\n",
        "    save_checkpoint(model, optimizer, epoch, current_epoch_metrics, latest_model_path)\n",
        "\n",
        "    # Determine the current metric value used for tracking the \"best\" model\n",
        "    # The code currently uses ROUGE-L from standard metrics (calculated every epoch)\n",
        "    current_metric_value = metrics_std['rouge-l']\n",
        "\n",
        "    # Check if the current model is the best seen so far based on best_val_metric\n",
        "    if current_metric_value > best_val_metric:\n",
        "        best_val_metric = current_metric_value # Update the best metric value\n",
        "        best_model_epoch = epoch # Store the epoch number of the new best model\n",
        "        print(f\"\\nNew best model found! Metric: {best_val_metric:.4f} at epoch {epoch+1}\")\n",
        "        # Save a separate checkpoint specifically for the best model\n",
        "        # Save the full set of metrics calculated in this epoch with the best model checkpoint\n",
        "        save_checkpoint(model, optimizer, epoch, current_epoch_metrics, best_model_path) # Save full metrics with best model\n",
        "    else:\n",
        "         # Ensure best_model_epoch is correctly reflected if a checkpoint was loaded initially\n",
        "         if best_model_epoch == -1: # Case where no best model was found initially\n",
        "              print(f\"Current model metric ({current_metric_value:.4f}). No best model found yet.\")\n",
        "         else:\n",
        "              print(f\"Current model metric ({current_metric_value:.4f}) is not better than best ({best_val_metric:.4f} at epoch {best_model_epoch+1}).\")\n",
        "\n",
        "\n",
        "    # Save a backup checkpoint periodically (e.g., every 5 epochs)\n",
        "    backup_interval = 5 # Define backup interval\n",
        "    if (epoch + 1) % backup_interval == 0:\n",
        "        backup_path = os.path.join(CHECKPOINT_DIR, f\"backup_epoch_{epoch+1}.pt\")\n",
        "        save_checkpoint(model, optimizer, epoch, current_epoch_metrics, backup_path)\n",
        "\n",
        "\n",
        "    # Save the entire metrics history to the JSON file after each epoch\n",
        "    # This happens every epoch to ensure consistent list lengths\n",
        "    save_metrics(all_metrics, METRICS_FILE)\n",
        "\n",
        "\n",
        "    # Clean up memory before the next epoch\n",
        "    del current_train_dataset # Delete the dataset object to free memory\n",
        "    gc.collect()\n",
        "    if device == torch.device('cuda'): torch.cuda.empty_cache()\n",
        "\n",
        "# Print message after training loop completes\n",
        "print(\"\\n===== Training Completed =====\")\n",
        "\n",
        "# Calculate and print the total training duration (excluding time spent on crashes/restarts)\n",
        "# This sum includes the runtime of each successfully completed epoch\n",
        "total_training_seconds = sum(all_metrics.get('epoch_runtime_seconds', []))\n",
        "total_training_minutes = total_training_seconds / 60.0\n",
        "print(f\"Total training duration (successful epochs): {total_training_minutes:.2f} minutes.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdylSQpo-FNr"
      },
      "outputs": [],
      "source": [
        "# (Cell 13) Final Test Set Evaluation\n",
        "\n",
        "print(\"\\n===== Evaluating on Test Set using Best Model =====\")\n",
        "test_eval_start_time = time.time() # Record the start time of test evaluation\n",
        "\n",
        "# Load the best performing model for the final evaluation\n",
        "# Prioritize the best model checkpoint\n",
        "best_model_path = os.path.join(DRIVE_PATH, \"best_model.pt\")\n",
        "latest_model_path = os.path.join(DRIVE_PATH, \"latest_model.pt\")\n",
        "\n",
        "if os.path.exists(best_model_path):\n",
        "    print(f\"Loading best model from {best_model_path} for final evaluation...\")\n",
        "    # Load the model state dictionary. Pass None for the optimizer as it's not needed for evaluation.\n",
        "    model, _, _, _ = load_checkpoint(best_model_path, model, None, device)\n",
        "elif os.path.exists(latest_model_path):\n",
        "    print(\"Warning: Best model checkpoint not found. Evaluating with the latest model.\")\n",
        "    # Load the latest model checkpoint if the best is not available\n",
        "    model, _, _, _ = load_checkpoint(latest_model_path, model, None, device)\n",
        "else:\n",
        "    # If neither checkpoint is found, print an error and exit or handle appropriately\n",
        "    print(\"Error: No model checkpoint found (best or latest) for final evaluation.\")\n",
        "    # exit() # Exit the script\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Lists to store generated summaries, references, and raw inputs for test evaluation\n",
        "test_references = [] # TF-IDF Targets for ROUGE/BLEU\n",
        "test_hypotheses = [] # Model generated summaries\n",
        "test_raw_inputs = [] # Original Texts for Clinical (Entity) metrics\n",
        "\n",
        "print(\"Running inference on the test set...\")\n",
        "# Use tqdm for a progress bar over the test batches\n",
        "progress_bar_test = tqdm(test_dataloader, desc=\"Testing\")\n",
        "\n",
        "# Disable gradient calculation during inference\n",
        "with torch.no_grad():\n",
        "    for batch in progress_bar_test:\n",
        "        # Move batch data to the appropriate device\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        # Get raw texts from the batch (these are not tensors)\n",
        "        raw_input_texts = batch['raw_input']\n",
        "        raw_target_texts = batch['raw_target']\n",
        "\n",
        "        # --- Generate Summaries for Test Samples ---\n",
        "        # Use the same generation parameters as used during validation\n",
        "        generated_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=generation_parameters[\"max_length\"],\n",
        "            num_beams=generation_parameters[\"num_beams\"],\n",
        "            length_penalty=generation_parameters[\"length_penalty\"],\n",
        "            early_stopping=generation_parameters[\"early_stopping\"],\n",
        "            no_repeat_ngram_size=generation_parameters[\"no_repeat_ngram_size\"]\n",
        "        )\n",
        "\n",
        "        # Decode the generated token IDs back into human-readable text\n",
        "        hypotheses_batch = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "        # Store the generated summaries, raw targets, and raw inputs for metric calculation later\n",
        "        test_hypotheses.extend(hypotheses_batch)\n",
        "        test_references.extend(raw_target_texts) # TF-IDF targets are references for standard metrics\n",
        "        test_raw_inputs.extend(raw_input_texts)  # Original inputs are references for clinical metrics\n",
        "\n",
        "\n",
        "        # Clean up batch variables and clear GPU cache\n",
        "        del input_ids, attention_mask, raw_input_texts, raw_target_texts, generated_ids\n",
        "        if device == torch.device('cuda'):\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect() # Collect garbage\n",
        "\n",
        "\n",
        "# --- Record Test Evaluation Runtime ---\n",
        "test_eval_end_time = time.time() # Record the end time of test evaluation\n",
        "test_eval_duration_seconds = test_eval_end_time - test_eval_start_time\n",
        "test_eval_duration_minutes = test_eval_duration_seconds / 60.0\n",
        "print(f\"Test set evaluation duration: {test_eval_duration_minutes:.2f} minutes.\")\n",
        "\n",
        "\n",
        "# --- Calculate Final Test Metrics ---\n",
        "print(\"\\nCalculating Final Test Metrics...\")\n",
        "\n",
        "# Calculate standard metrics (ROUGE, BLEU) comparing generated summaries to TF-IDF targets\n",
        "final_metrics_std = calculate_metrics(test_references, test_hypotheses)\n",
        "print(f\"Final Standard Metrics - R1: {final_metrics_std['rouge-1']:.4f}, R2: {final_metrics_std['rouge-2']:.4f}, RL: {final_metrics_std['rouge-l']:.4f}, B: {final_metrics_std['bleu']:.4f}\")\n",
        "\n",
        "# Calculate clinical metrics (Entity Overlap) comparing generated summaries to original raw inputs\n",
        "final_metrics_clin = calculate_clinical_metrics(test_raw_inputs, test_hypotheses)\n",
        "print(f\"Final Clinical Metrics - ER: {final_metrics_clin['entity_recall']:.4f}, EP: {final_metrics_clin['entity_precision']:.4f}, EF1: {final_metrics_clin['entity_f1']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSuftXHc-WL8"
      },
      "outputs": [],
      "source": [
        "# (Cell 14) Save Final Results and Model\n",
        "\n",
        "# --- Save Final Results Summary ---\n",
        "# Combine all final test metrics and relevant information into a summary dictionary\n",
        "final_results_summary = {\n",
        "    'test_metrics_standard': final_metrics_std,\n",
        "    'test_metrics_clinical': final_metrics_clin,\n",
        "    'training_history_file': METRICS_FILE, # Path to the full training history JSON\n",
        "    'training_config_file': CONFIG_FILE, # Path to the training configuration JSON\n",
        "    'qualitative_examples_file': QUALITATIVE_EXAMPLES_FILE, # Path to the qualitative examples JSON\n",
        "    'best_model_epoch': best_model_epoch + 1 if best_model_epoch != -1 else 'N/A', # +1 for human-readable epoch number\n",
        "    'best_model_validation_metric': best_val_metric, # The best validation metric achieved\n",
        "    'final_training_samples_used': all_metrics['sample_sizes'][-1] if all_metrics['sample_sizes'] else 'N/A', # Size of training data in the last epoch\n",
        "    'total_training_duration_minutes': total_training_minutes, # Total time spent in training loop\n",
        "    'test_evaluation_duration_minutes': test_eval_duration_minutes # Total time spent evaluating on test set\n",
        "}\n",
        "\n",
        "print(f\"\\nSaving final results summary to {FINAL_RESULTS_FILE}...\")\n",
        "try:\n",
        "    # Save the summary dictionary to a JSON file\n",
        "    with open(FINAL_RESULTS_FILE, 'w', encoding='utf-8') as f:\n",
        "        json.dump(final_results_summary, f, ensure_ascii=False, indent=4)\n",
        "    print(\"Final results summary saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving final results summary: {e}\")\n",
        "\n",
        "# --- Save the Final Trained Model ---\n",
        "# Save the complete fine-tuned model using save_pretrained\n",
        "# This saves the model's architecture and weights in a format that can be easily reloaded\n",
        "print(f\"\\nSaving the complete fine-tuned model (from best epoch) to {FINAL_MODEL_PATH}...\")\n",
        "try:\n",
        "    # Ensure the directory for the final model exists\n",
        "    os.makedirs(FINAL_MODEL_PATH, exist_ok=True)\n",
        "    model.save_pretrained(FINAL_MODEL_PATH)\n",
        "    # It's good practice to also save the tokenizer with the model,\n",
        "    # though it was already saved earlier, doing it again here ensures they are together.\n",
        "    tokenizer.save_pretrained(FINAL_MODEL_PATH)\n",
        "    print(\"Final model and tokenizer saved successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving final model: {e}\")\n",
        "\n",
        "\n",
        "# --- Save Qualitative Examples ---\n",
        "# Select a small number of random examples from the test set results\n",
        "print(f\"\\nSelecting and saving qualitative examples to {QUALITATIVE_EXAMPLES_FILE}...\")\n",
        "num_examples_to_save = 10 # Define how many examples to save\n",
        "saved_examples = [] # List to store the selected example dictionaries\n",
        "\n",
        "# Ensure there are test samples available\n",
        "if len(test_raw_inputs) > 0:\n",
        "    # Select random indices from the range of test samples\n",
        "    # Use min() to handle cases where test set size is less than num_examples_to_save\n",
        "    selected_indices = random.sample(range(len(test_raw_inputs)), min(num_examples_to_save, len(test_raw_inputs)))\n",
        "\n",
        "    # Iterate through the selected indices and create a dictionary for each example\n",
        "    for idx in selected_indices:\n",
        "        saved_examples.append({\n",
        "            \"original_input\": test_raw_inputs[idx], # The original full report text\n",
        "            \"tfidf_target\": test_references[idx], # The TF-IDF extractive summary (training target)\n",
        "            \"generated_summary\": test_hypotheses[idx] # The summary generated by the model\n",
        "        })\n",
        "\n",
        "    try:\n",
        "        # Save the list of example dictionaries to a JSON file\n",
        "        with open(QUALITATIVE_EXAMPLES_FILE, 'w', encoding='utf-8') as f:\n",
        "            json.dump(saved_examples, f, ensure_ascii=False, indent=4)\n",
        "        print(f\"{len(saved_examples)} qualitative examples saved successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving qualitative examples: {e}\")\n",
        "else:\n",
        "    print(\"No test samples available to save qualitative examples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyVR7lLV-JXz"
      },
      "outputs": [],
      "source": [
        "# (Cell 15) Example Inference\n",
        "\n",
        "# --- Example Inference Function ---\n",
        "# This function demonstrates how to use the fine-tuned model to generate a summary for a new raw input text.\n",
        "# It applies the same preprocessing steps (section tokens, length token) as used during training.\n",
        "\n",
        "def generate_summary(input_text_raw, model, tokenizer, device, max_gen_length=generation_parameters[\"max_length\"]):\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # --- Apply Preprocessing Steps as done during training (Crucial) ---\n",
        "    # The inference input MUST be formatted consistently with the training input.\n",
        "\n",
        "    # 1. Add Section Tokens\n",
        "    section_aware_text = add_section_tokens(input_text_raw, section_headers_map=SECTION_HEADERS, all_headers_regex=ALL_HEADERS_REGEX)\n",
        "\n",
        "\n",
        "    # Using Option A for this example: prepend a default length token.\n",
        "    # Choose one of your defined length control tokens.\n",
        "    control_token_for_inference = \"<SUM_MEDIUM>\" # Example default for inference\n",
        "\n",
        "\n",
        "    # 2. Create the final formatted input text\n",
        "    # The format must match training: <LENGTH_TOKEN> <SECTION_AWARE_TEXT>\n",
        "    final_input_text = control_token_for_inference + \" \" + section_aware_text\n",
        "\n",
        "    # Add a basic check for empty input text after processing\n",
        "    if not final_input_text.strip():\n",
        "         print(\"Warning: Input text is empty after preprocessing. Cannot generate summary.\")\n",
        "         return \"Error: Empty input text after preprocessing.\"\n",
        "\n",
        "\n",
        "    # --- Tokenize the preprocessed input text ---\n",
        "    encoding = tokenizer(\n",
        "        final_input_text,\n",
        "        max_length=MAX_INPUT_LENGTH, # Use the same max input length as training\n",
        "        padding='max_length', # Pad to max_length\n",
        "        truncation=True, # Truncate if longer than max_length\n",
        "        return_tensors=\"pt\" # Return PyTorch tensors\n",
        "    )\n",
        "\n",
        "    # Move tokenized input to the appropriate device\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "\n",
        "    summary_text = \"Error generating summary.\" # Default error message in case of failure\n",
        "\n",
        "    # --- Generate Summary using the model ---\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        try:\n",
        "            # Generate summary using the same parameters as validation/test evaluation\n",
        "            generated_ids = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_length=max_gen_length, # Use the specified max generation length\n",
        "                num_beams=generation_parameters[\"num_beams\"],\n",
        "                length_penalty=generation_parameters[\"length_penalty\"],\n",
        "                early_stopping=generation_parameters[\"early_stopping\"],\n",
        "                no_repeat_ngram_size=generation_parameters[\"no_repeat_ngram_size\"]\n",
        "            )\n",
        "            # Decode the generated IDs back to text, skipping special tokens\n",
        "            summary_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during summary generation: {e}\")\n",
        "\n",
        "\n",
        "    # Clean up tensors and clear GPU cache\n",
        "    del input_ids, attention_mask, encoding\n",
        "    if device == torch.device('cuda'):\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect() # Collect garbage\n",
        "\n",
        "    return summary_text\n",
        "\n",
        "\n",
        "# --- Example Usage of Inference Function ---\n",
        "print(\"\\n--- Example Inference ---\")\n",
        "\n",
        "# Define a sample raw input text (like a new radiology report)\n",
        "sample_text_raw = \"\"\"\n",
        "INDICATION: Evaluate for pneumonia.\n",
        "TECHNIQUE: Portable anteroposterior chest X-ray.\n",
        "FINDINGS: The lungs are clear bilaterally without evidence of consolidation, effusion, or pneumothorax. The heart size is normal. Mediastinal and hilar contours are unremarkable. Visualized osseous structures are intact.\n",
        "IMPRESSION: No acute cardiopulmonary abnormality.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Generate a summary for the sample raw text\n",
        "generated_text = generate_summary(sample_text_raw, model, tokenizer, device)\n",
        "\n",
        "# Print the original text and the generated summary\n",
        "print(f\"Input Text (Raw):\\n{sample_text_raw}\\n\")\n",
        "print(f\"Generated Summary:\\n{generated_text}\")\n",
        "\n",
        "print(\"\\n===== Script Finished =====\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}